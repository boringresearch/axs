{
    "mlperf_inference_git_entry": [ "^", "byquery", "git_repo,repo_name=mlperf_inference_git" ],
    "bert_code_extra_path": [ "^^", "execute", [[
        [ "get", "mlperf_inference_git_entry" ],
        [ "get_path", [[ "language","bert", "DeepLearningExamples", "TensorFlow", "LanguageModeling", "BERT" ]] ]
    ]] ],

    "abs_script_path": [ "^^", "execute", [[
        [ "get", "mlperf_inference_git_entry" ],
        [ "get_path", [[ "language","bert", "accuracy-squad.py" ]] ]
            ]], {}, ["mlperf_inference_git_entry"] ],

    "accuracy_log_path": ["^^", "get_path", "accuracy_log.json"],
    "check_file_exist": [ "^^", "func", [ "os.path.isfile" , ["^^", "get", "accuracy_log_path"] ] ],
    "guaranteed_accuracy_log_path": [ "^^", "case",[ ["^^",  "get", "check_file_exist"], "False", ["^^", "generate_accuracy_log"]] , {"default_value":  ["^^", "get", "accuracy_log_path"] } ],

    "predictions_path":   [ "^^", "get_path", "predictions.json" ],
        
    "encoded_accuracy_log": [ "^^", "get", "get_accuracy_log" ],
   
    "squad_dataset_original_path": [ "^^", "dig", "script_output.squad_dataset_original_path" ],
    "squad_dataset_tokenized_path": [ "^^", "dig", "script_output.squad_dataset_tokenized_path" ],
    "selected_size": [ "^^", "dig", "script_output.selected_size" ],

    "accuracy": [ "^^", "execute", [[
        [ "get_kernel" ],
        [ "byname", "python_script" ],
        [ "run", [], {
                "python_deps": [
                    [ "^^", "get", "bert_code_extra_path" ],
                    [ "^", "byquery", "python_package,package_name=tensorflow" ]
                ],
                "abs_script_path": ["^^", "get", "abs_script_path"],
                "script_extra_params": [ "^^", "substitute", "--vocab_file=\"#{tokenization_vocab_path}#\" --val_data=\"#{squad_dataset_original_path}#\" --features_cache_file=\"#{squad_dataset_tokenized_path}#\" --log_file=\"#{guaranteed_accuracy_log_path}#\" --out_file=\"#{predictions_path}#\" --max_examples=\"#{selected_size}#\" " ],
                "capture_output": true
            } ]
    ]], {} ],

    "script_output": [ "^^", "execute", [[
        [ "get_path_from", "output_file_name" ],
        0,
        [ "func", "ufun.load_json" ]
    ]] ],

    "tokenization_vocab_query":[ "downloaded","tokenization","vocab_for_bert" ],
    "tokenization_vocab_entry": [ "^", "byquery", [[ "^^", "get", "tokenization_vocab_query" ]], {}, ["tokenization_vocab_query"] ],
    "tokenization_vocab_path": [ "^^", "execute", [[
        [ "get", "tokenization_vocab_entry" ],
        [ "get_path" ]
    ]] ],
    
    "output_logits": [ "^^", "dig", "script_output.output_logits" ]
}

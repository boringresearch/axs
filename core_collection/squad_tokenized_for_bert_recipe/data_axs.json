 {
    "_parent_entries": [ [ "^", "byname", "python_script" ] ],
    "_producer_rules": [
        [ [ "tokenized", "squad_v1_1" ], [["run"]], {}, ["rel_script_path", "script_extra_params"] ]
    ],

    "return_saved_record_entry": false,
    "return_this_entry": [ "^^", "execute", [[
        [ "get_kernel" ],
        [ "work_collection" ],
        [ "attached_entry", [ "tokenized_squad_v1_1", {
            "_parent_entries": [ "AS^IS", [ "^", "byname", "base_experiment" ] ],
            "tags": ["tokenized", "squad_v1_1"],
            "file_name": "tokenized_squad_v1.1.pickled"
         } ] ],
         [ "save" ]
    ]] ],
    "tokenized_squad_path": [ "^^", "execute", [[
        [ "get", "return_this_entry" ],
        [ "get_path" ]
    ]] ],

    "tensorflow_query": ["python_package", "package_name=tensorflow"],
    "absl_query": ["python_package", "package_name=absl-py"],
    "transformers_query": ["python_package", "package_name=transformers"],

    "python_deps": [
        [ "^", "byquery", [ [ "^^", "get", "absl_query" ] ], {}, ["absl_query"] ],
        [ "^", "byquery", [ [ "^^", "get", "tensorflow_query" ] ], {}, ["tensorflow_query"] ],
        [ "^", "byquery", [ [ "^^", "get", "transformers_query" ] ], {}, ["transformers_query"] ]
    ],
    "mlperf_inference_git_entry": [ "^", "byquery", "git_repo,repo_name=mlperf_inference_git" ],

    "mlperf_inference_path": [ "^^", "execute", [[
        [ "get", "mlperf_inference_git_entry" ],
        [ "get_path" ]
    ]], {}, ["mlperf_inference_git_entry"] ],

    "squad_dataset_query":[ "downloaded", "squad_original" ],
    "squad_dataset_entry": [ "^", "byquery", [[ "^^", "get", "squad_dataset_query" ]], {}, ["squad_dataset_query"] ],
    "squad_dataset_original_path": [ "^^", "execute", [[
        [ "get", "squad_dataset_entry" ],
        [ "get_path" ]
    ]] ],

    "squad_tokenization_vocab_query":[ "downloaded", "tokenization", "vocab_for_bert" ],
    "squad_tokenization_vocab_entry": [ "^", "byquery", [[ "^^", "get", "squad_tokenization_vocab_query" ]], {}, ["squad_tokenization_vocab_query"] ],
    "squad_tokenization_vocab_path": [ "^^", "execute", [[
        [ "get", "squad_tokenization_vocab_entry" ],
        [ "get_path" ]
    ]] ],
    "dataset_max_seq_length": "384",
    "dataset_max_query_length": "64",
    "dataset_doc_stride": "128",
    "dataset_raw": "no",
    "rel_script_path": "tokenize_and_pack.py",
    "script_extra_params": [ "^^", "substitute", "\"#{squad_dataset_original_path}#\" \"#{squad_tokenization_vocab_path}#\" \"#{tokenized_squad_path}#\" #{dataset_max_seq_length}# \"#{dataset_max_query_length}#\" \"#{dataset_doc_stride}#\" \"#{dataset_raw}#\" \"#{mlperf_inference_path}#\"" ]
}
